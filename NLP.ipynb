{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Natural Language Processing!\n",
    "\n",
    "# First, let's look at the Bag of Words-Model.\n",
    "\n",
    "# We want to import some Libraries from sklearn and nltk that help us with this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "import re\n",
    "import pandas as pd\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next we want to use a simple string as our test-content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\"\n",
    "    \"It's exciting to watch flying fish after a hard day's work. I don't know why some fish prefer flying and other fish would rather swim. It seems like the fish just woke up one day and decided, 'hey, today is the day to fly away.'\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we're making a function that tokenizes and cleans this data for us so we can get rid of some noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_clean_data(content):\n",
    "    sentences = nltk.sent_tokenize(content)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    corpus = []\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        sent = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "        sent = sent.lower()\n",
    "        sent= sent.split()\n",
    "        sent = [lemmatizer.lemmatize(word) for word in sent if not word in set(stopwords.words('english'))]\n",
    "        sent = ' '.join(sent)   \n",
    "        corpus.append(sent)\n",
    "    return corpus\n",
    "\n",
    "tokenized_data = tokenize_and_clean_data(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next up we're using this cleaned up token and create our Bag of Words Model.\n",
    "# To be able to do that, we need to initialize the tokenizer and run it with the token we just created. (To simplify things, I initialized all needed Vectorizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exciting</th>\n",
       "      <th>watch</th>\n",
       "      <th>flying</th>\n",
       "      <th>fish</th>\n",
       "      <th>hard</th>\n",
       "      <th>day</th>\n",
       "      <th>work</th>\n",
       "      <th>know</th>\n",
       "      <th>prefer</th>\n",
       "      <th>would</th>\n",
       "      <th>...</th>\n",
       "      <th>swim</th>\n",
       "      <th>seems</th>\n",
       "      <th>like</th>\n",
       "      <th>woke</th>\n",
       "      <th>one</th>\n",
       "      <th>decided</th>\n",
       "      <th>hey</th>\n",
       "      <th>today</th>\n",
       "      <th>fly</th>\n",
       "      <th>away</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   exciting  watch  flying  fish  hard  day  work  know  prefer  would  ...  \\\n",
       "0         0      1       0     1     1    0     1     1       0      0  ...   \n",
       "1         0      0       0     0     2    0     1     0       0      1  ...   \n",
       "2         1      2       1     0     1    1     0     0       1      0  ...   \n",
       "\n",
       "   swim  seems  like  woke  one  decided  hey  today  fly  away  \n",
       "0     0      0     0     0    0        0    1      0    1     0  \n",
       "1     0      1     1     0    1        0    0      0    0     1  \n",
       "2     1      0     0     1    0        1    0      1    0     0  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "tfidf = TfidfVectorizer(norm=None)\n",
    "tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "# Im converting the tokenized sentences into a list of words to be able to make a dataframe out of it\n",
    "tokenized_data_words = \" \".join(tokenized_data).split(\" \")\n",
    "test_data_list = list()\n",
    "for word in tokenized_data_words:\n",
    "    if not word in test_data_list:\n",
    "        test_data_list.append(word)\n",
    "\n",
    "# Here we create the Bag_of_Words model\n",
    "bow_model = cv.fit_transform(tokenized_data).toarray()\n",
    "bow_model_df = pd.DataFrame(bow_model, columns=test_data_list)\n",
    "bow_model_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's look at the bag of words in a sorted DataFrame and see which words were used the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fish</th>\n",
       "      <th>day</th>\n",
       "      <th>flying</th>\n",
       "      <th>exciting</th>\n",
       "      <th>watch</th>\n",
       "      <th>hard</th>\n",
       "      <th>work</th>\n",
       "      <th>know</th>\n",
       "      <th>prefer</th>\n",
       "      <th>would</th>\n",
       "      <th>...</th>\n",
       "      <th>swim</th>\n",
       "      <th>seems</th>\n",
       "      <th>like</th>\n",
       "      <th>woke</th>\n",
       "      <th>one</th>\n",
       "      <th>decided</th>\n",
       "      <th>hey</th>\n",
       "      <th>today</th>\n",
       "      <th>fly</th>\n",
       "      <th>away</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   fish  day  flying  exciting  watch  hard  work  know  prefer  would  ...  \\\n",
       "0     4    3       2         1      1     1     1     1       1      1  ...   \n",
       "\n",
       "   swim  seems  like  woke  one  decided  hey  today  fly  away  \n",
       "0     1      1     1     1    1        1    1      1    1     1  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_sorted_df(tokenized_data):\n",
    "    tokenized_list = \" \".join(tokenized_data).split(\" \")\n",
    "    bow_dict = dict()\n",
    "    for word in tokenized_list:\n",
    "        if word not in bow_dict:\n",
    "            bow_dict[word] = 1\n",
    "        else:\n",
    "            bow_dict[word] += 1\n",
    "    sorted_df = pd.DataFrame([dict(sorted(bow_dict.items(), key=lambda item: item[1], reverse=True))])\n",
    "    return sorted_df\n",
    "\n",
    "df_sorted = create_sorted_df(tokenized_data)\n",
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next up let's look at the tfidf model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exciting</th>\n",
       "      <th>watch</th>\n",
       "      <th>flying</th>\n",
       "      <th>fish</th>\n",
       "      <th>hard</th>\n",
       "      <th>day</th>\n",
       "      <th>work</th>\n",
       "      <th>know</th>\n",
       "      <th>prefer</th>\n",
       "      <th>would</th>\n",
       "      <th>...</th>\n",
       "      <th>swim</th>\n",
       "      <th>seems</th>\n",
       "      <th>like</th>\n",
       "      <th>woke</th>\n",
       "      <th>one</th>\n",
       "      <th>decided</th>\n",
       "      <th>hey</th>\n",
       "      <th>today</th>\n",
       "      <th>fly</th>\n",
       "      <th>away</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.693147</td>\n",
       "      <td>2.575364</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   exciting     watch    flying      fish  hard       day      work      know  \\\n",
       "0  0.000000  1.287682  0.000000  1.693147   1.0  0.000000  1.287682  1.693147   \n",
       "1  0.000000  0.000000  0.000000  0.000000   2.0  0.000000  1.287682  0.000000   \n",
       "2  1.693147  2.575364  1.693147  0.000000   1.0  1.693147  0.000000  0.000000   \n",
       "\n",
       "     prefer     would  ...      swim     seems      like      woke       one  \\\n",
       "0  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  1.693147  ...  0.000000  1.693147  1.693147  0.000000  1.693147   \n",
       "2  1.693147  0.000000  ...  1.693147  0.000000  0.000000  1.693147  0.000000   \n",
       "\n",
       "    decided       hey     today       fly      away  \n",
       "0  0.000000  1.693147  0.000000  1.693147  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.000000  1.693147  \n",
       "2  1.693147  0.000000  1.693147  0.000000  0.000000  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow_model_tfidf = pd.DataFrame(tfidf.fit_transform(tokenized_data).toarray(), columns=test_data_list)\n",
    "df_bow_model_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can see here very clearly the similarities between these two models.\n",
    "\n",
    "# Furthermore, the most important words to a text, or also called keywords, are the ones with the highest values and show the importance of those words in the sentence.\n",
    "\n",
    "# One problem we have though is, that these kind of BoW-Models do not take word context into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try getting word context by looking at a very famous book: \"The Adventures of Sherlock Holmes\" by Arthur Conan Doyle.\n",
    "\n",
    "# Due to the great ressource \"www.gutenberg.org\", where a very big amount of text can be used for these kind of things, I will import the text in here and look at which words are most similar to sherlock or other important figures in this novel.\n",
    "\n",
    "# Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26081449\n",
      "[('abroad', 0.9899264574050903), ('waylaid', 0.9898195266723633), ('mania', 0.9890733361244202), ('constitution', 0.9887520670890808), ('blasted', 0.9863550066947937), ('recently', 0.9863343834877014), ('admirers', 0.9858965873718262), ('finder', 0.9855631589889526), ('destroyed', 0.985058069229126), ('wronged', 0.9850126504898071), ('eton', 0.984745979309082), ('loading', 0.9845803380012512), ('ascertained', 0.9842536449432373), ('ragged', 0.984205961227417), ('suspended', 0.9840380549430847), ('definitely', 0.9837009310722351), ('cure', 0.9831808805465698), ('advertisement—how', 0.9829784631729126), ('tiniest', 0.9828833937644958), ('“five', 0.9828272461891174)]\n"
     ]
    }
   ],
   "source": [
    "# First we use our old trusty open-function to import the data into a string-variable:\n",
    "with open(\"SherlockHolmes.txt\", 'r+', encoding='utf-8') as file:\n",
    "    file_text = file.read()\n",
    "\n",
    "# Next we cut to the good part of the story and make sure we leave everything but the actual book content out:\n",
    "corpus = file_text[file_text.find(\"To Sherlock Holmes she is always the woman.\"):file_text.find(\"END OF THE PROJECT GUTENBERG EBOOK\") - 4].replace(\"\\n\", \" \")\n",
    "\n",
    "# Now we need to define two functions that preprocess the data for us:\n",
    "def process_corpus(corpus):\n",
    "    sentence_tokenized_corpus = tokenizer.tokenize(corpus)\n",
    "    returned_list = list()\n",
    "    helper_list = list()\n",
    "    for sentence in sentence_tokenized_corpus:\n",
    "        word_tokenized_sentence = [word.lower().strip('.').strip('?').strip('!') for word in sentence.replace(\",\",\"\").replace(\"-\",\" \").replace(\":\",\"\").split()]\n",
    "        helper_list.append(word_tokenized_sentence)\n",
    "        returned_list.append(helper_list)\n",
    "    return returned_list\n",
    "\n",
    "# This creates a list of lists, where the sentences are the inside lists. This is needed to use the gensim Model Function Word2Vec\n",
    "def create_list_of_lists(corpus):\n",
    "    all_sentences = list()\n",
    "    for input in corpus:\n",
    "        for sentence in input:\n",
    "            all_sentences.append(sentence)\n",
    "    return all_sentences\n",
    "\n",
    "# now we tokenize this book:\n",
    "word_tokenized_book = process_corpus(corpus)\n",
    "\n",
    "# and make the list of lists to be used as a trained model:\n",
    "all_sentences = create_list_of_lists(word_tokenized_book)\n",
    "\n",
    "# Finally we are ready to create our model with this book.\n",
    "# Since I don't want to wait forever, I'll only use the first 10000 words of this book.\n",
    "print(len(all_sentences))\n",
    "corpus_model = gensim.models.Word2Vec(all_sentences[:10000], vector_size=96, window=5, min_count=1, workers=3, sg=1)\n",
    "\n",
    "# Let's see which words are most similar to sherlock:\n",
    "similar_extraction = corpus_model.wv.most_similar(\"hudson\", topn=20)\n",
    "print(similar_extraction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0a4fa72f226d201fd69ad23848e279ccfb843ab38969d879c0da7a403b799c6e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
